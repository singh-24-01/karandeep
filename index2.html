<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <link rel="icon" href="/favicon-brain-neon.svg" type="image/svg+xml">
  <link rel="alternate icon" href="/favicon-brain-neon.svg">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Emploi du temps ‚Äì Master MLSD & AMSD</title>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>


</head>
<body>
   <canvas id="space" aria-hidden="true"></canvas>

  <!-- HEADER -->
  <header>
    <div class="wrap nav">
      <div class="brand"><div class="logo" aria-hidden="true"></div><div>Master <span style="color:var(--brand)">MLSD</span> & <span style="color:var(--brand-2)">AMSD</span></div></div>
      <nav class="links" aria-label="Navigation principale">
        <a href="#planning" class="active" data-section="planning">Planning</a>
        <a href="#calendar" data-section="calendar">Calendrier</a>
        <a href="#presentation" data-section="presentation">Pr√©sentation du Master</a>
        <a href="#terms" data-section="terms">Termes</a>
        <a href="#simulations" data-section="simulations">Simulations</a>
        <a href="#plan" data-section="plan">Plan</a>
    </nav>

      <div class="actions">
        <a class="btn" href="https://drive.google.com/drive/folders/1mcv9r4mTjnj8O9xhBEErvnfUtd0tkTrp?usp=sharing" target="_blank" rel="noopener">Drive</a>
        <a class="btn" href="https://intranet.u-paris.fr/" target="_blank" rel="noopener">Intranet</a>
        <a class="btn" href="https://moodle.u-paris.fr/" target="_blank" rel="noopener">Moodle</a>
        
        <button class="btn" id="themeToggle" aria-label="Basculer le th√®me">üåì</button>
      </div>
    </div>
  </header>
  <!-- === Prochains cours : popover === -->
  <div id="next-popover" class="popover" hidden>
    <div class="popover-inner">
      <div class="popover-title">√Ä venir</div>
      <ul id="next-list" class="popover-list"></ul>
    </div>
  </div>
  <!-- MAIN -->
  <main class="wrap">
    <!-- PLANNING -->
    <section id="planning" class="reveal">
      <div class="card">
        <div class="pill"><span class="dot"></span> Prochain cours <span id="nextTitle" style="margin-left:6px; font-weight:800"></span></div>
        <h1 class="h1">Emploi du temps ‚Äì Semaine en cours</h1>


        <div class="widget">
          <div class="row">
            <div class="seg">
              <div class="view-toggle" role="tablist" aria-label="Type de vue">
                <button class="chip active" data-mode="WEEK" role="tab" aria-selected="true">Semaine</button>
                <button class="chip" data-mode="AGENDA" role="tab" aria-selected="false">Agenda</button>
                <button class="chip" data-mode="MONTH" role="tab" aria-selected="false">Mois</button>
              </div>
            </div>
            <div class="seg" style="text-align:right">
              <button class="btn" id="prevWeek" title="Semaine pr√©c√©dente">‚óÄ</button>
              <button class="btn" id="today" title="Aujourd'hui">Aujourd'hui</button>
              <button class="btn" id="nextWeek" title="Semaine suivante">‚ñ∂</button>
            </div>
          </div>
          <div class="row">
            <div class="seg kbd">Timezone : Europe/Paris</div>
            <div class="seg" style="text-align:right"><span class="kbd">D√©but du prochain cours dans</span> <span class="countdown" id="countdown">‚Äî</span></div>
          </div>
          <div class="calendar-shell reveal d2">
            <iframe id="gcal" class="cal" title="Google Calendar embedded"></iframe>
          </div>
          <p class="sub">Si le calendrier ne s'affiche pas correctement, <a href="https://docs.google.com/spreadsheets/d/1kMMIjEAckDntNG2YaYYK5fmQFvvtYrhIX1LcbfpL7vY/edit?gid=0#gid=0" target="_blank" rel="noopener">consulte la version de secours</a>.</p>
        </div>
      </div>
    </section>

    <section id="calendar" class="reveal d2" hidden>
      <div class="card">
        <h2 class="h1">Calendrier de formation 2025‚Äì2026</h2>
        <img src="calendrier2025.png" alt="Calendrier MLSD FI & FA " style="width:100%; border-radius:14px; border:1px solid var(--card-border)"/>
        <p class="sub">Valable pour MLSD FI & FA (2025‚Äì2026).</p>
      </div>
    </section>

    <section id="presentation" class="reveal d2" hidden>
        <div class="card">
            <h2 class="h1">Pr√©sentation du Master</h2>
            <p class="sub">Document officiel 2025‚Äì2026.</p>

        <iframe
            src="PreÃÅsentation_Master_M2_MLSD-25-26.pdf"
            width="100%"
            height="600"
            style="border-radius:14px; border:1px solid var(--card-border)"
        ></iframe>

            <p style="margin-top:12px">
                <a class="btn" href="PreÃÅsentation_Master_M2_MLSD-25-26.pdf" target="_blank" rel="noopener">
                üìÑ T√©l√©charger le PDF
                </a>
            </p>
        </div>
    </section>

    <section id="terms" class="reveal d2" hidden>
  <div class="card">
    <h2 class="h1">Termes statistiques essentiels</h2>
    <div class="terms-filters">
  <button data-cat="all" class="active">üåê Tout</button>
  <button data-cat="stats">üìä Statistiques de base</button>
  <button data-cat="proba">üéØ Probabilit√©s</button>
  <button data-cat="regression">üìà R√©gression</button>
  <button data-cat="ml">ü§ñ Machine Learning</button>
  <button data-cat="algebra">üî¢ Alg√®bre lin√©aire</button>
  <button data-cat="timeseries">üïí S√©ries temporelles</button>
  <button data-cat="abtest">üß™ A/B testing</button>

</div>

    <div class="terms-search">
  <input id="termsSearch" type="search"
         placeholder="Rechercher un terme (ex : variance, Poisson, ROC)‚Ä¶"
         autocomplete="off" />
  <span class="kbd" id="termsCount"></span>
</div>
    <div class="terms-content">
      <p data-cat="stats"><strong>Variance :</strong> Mesure de la dispersion des donn√©es autour de la moyenne. Plus la variance est grande, plus les donn√©es sont dispers√©es.</p>
      <p data-cat="algebra"><strong>Valeurs propres (Eigenvalues) :</strong> Les scalaires associ√©s aux vecteurs propres d'une matrice, d√©crivant dans quelle mesure ces vecteurs sont √©tir√©s ou compress√©s par la transformation lin√©aire.</p>
      <p data-cat="stats"><strong>Biais (Bias) :</strong> Diff√©rence entre la valeur attendue d'un estimateur et la vraie valeur d'un param√®tre. Un mod√®le biais√© peut manquer de pr√©cision.</p>
      <p data-cat="stats"><strong>√âcart-type (Standard Deviation) :</strong> Racine carr√©e de la variance. Il exprime la dispersion des donn√©es par rapport √† la moyenne dans la m√™me unit√© que les donn√©es.</p>
      <p data-cat="stats"><strong>Quartiles :</strong> Valeurs qui divisent un ensemble de donn√©es en quatre parties √©gales. Le premier quartile (Q1), la m√©diane (Q2), et le troisi√®me quartile (Q3).</p>
      <p data-cat="proba"><strong>Loi de Bernoulli :</strong> Loi de probabilit√© qui d√©crit une exp√©rience al√©atoire ayant deux issues possibles (succ√®s (p) ou √©chec (1-p)), avec une probabilit√© fixe de succ√®s.</p>
      <p data-cat="proba"><strong>Loi normale (Gaussian Distribution) :</strong> Loi de probabilit√© continue sym√©trique en forme de cloche, d√©finie par une moyenne Œº et un √©cart-type œÉ, tr√®s utilis√©e pour mod√©liser des ph√©nom√®nes naturels et de nombreuses approximations statistiques.</p>
      <p data-cat="proba"><strong>Loi de Poisson :</strong> Distribution de probabilit√© discr√®te qui mod√©lise le nombre d'√©v√©nements se produisant dans un intervalle de temps ou d'espace fixe, lorsque ces √©v√©nements surviennent ind√©pendamment et √† un taux constant.</p>
      <p data-cat="stats"><strong>Esp√©rance math√©matique (Expected Value) :</strong> Moyenne pond√©r√©e des r√©sultats d'une variable al√©atoire. C'est la valeur centrale attendue d'une exp√©rience al√©atoire.</p>
      <p data-cat="proba"><strong>P-value :</strong> Probabilit√©, sous l‚Äôhypoth√®se nulle, d‚Äôobserver une statistique de test au moins aussi extr√™me que celle mesur√©e. Utilis√©e pour √©valuer la significativit√© statistique d‚Äôun r√©sultat.</p>
      <p data-cat="stats"><strong>Intervalle de confiance (Confidence Interval) :</strong> Plage de valeurs qui, avec une certaine probabilit√©, contient la vraie valeur d'un param√®tre statistique.</p>
      <p data-cat="regression"><strong>R√©gression lin√©aire :</strong> Technique statistique utilis√©e pour mod√©liser la relation entre une variable d√©pendante et une ou plusieurs variables ind√©pendantes.</p>
      <p data-cat="ml"><strong>Surapprentissage (Overfitting) :</strong> Situation o√π un mod√®le est trop ajust√© aux donn√©es d'entra√Ænement, ce qui nuit √† sa g√©n√©ralisation sur des donn√©es nouvelles.</p>
      <p data-cat="ml"><strong>Sous-apprentissage (Underfitting) :</strong> Mod√®le trop simple pour capturer les relations sous-jacentes dans les donn√©es, conduisant √† une mauvaise performance.</p>
      <p data-cat="ml,stats"><strong>Matrice de confusion :</strong> Tableau utilis√© pour √©valuer la performance d'un mod√®le de classification en comptant les vraies et fausses pr√©dictions.</p>
      <p data-cat="stats"><strong>Test d'hypoth√®se :</strong> M√©thode statistique pour tester une affirmation sur un param√®tre de population en utilisant les donn√©es d'un √©chantillon.</p>
      <p data-cat="stats"><strong>Bootstrap :</strong> Technique de r√©√©chantillonnage qui permet d'estimer les propri√©t√©s d'un estimateur en r√©√©chantillonnant avec remplacement.</p>
      <p data-cat="stats"><strong>AIC/BIC :</strong> Crit√®res utilis√©s pour comparer diff√©rents mod√®les statistiques en combinant qualit√© d‚Äôajustement et p√©nalisation de la complexit√© du mod√®le.</p>
      <p data-cat="ml,stats"><strong>Analyse en composantes principales (ACP) :</strong> M√©thode de r√©duction de dimensionnalit√© qui transforme des variables corr√©l√©es en nouvelles variables non corr√©l√©es tout en conservant l'essentiel de la variance.</p>
      <p data-cat="algebra"><strong>Matrice :</strong> Un tableau de nombres dispos√©s en lignes et en colonnes, utilis√© pour organiser des ensembles de donn√©es et des transformations lin√©aires.</p>
      <p data-cat="stats,algebra"><strong>Covariance :</strong> Mesure de la mani√®re dont deux variables varient ensemble. Si la covariance est positive, les deux variables augmentent ensemble, si elle est n√©gative, elles varient en sens inverse.</p>
      <p data-cat="algebra"><strong>Matrice de covariance :</strong> Matrice carr√©e qui pr√©sente les covariances entre chaque paire de variables dans un ensemble de donn√©es.</p>
      <p data-cat="algebra"><strong>Vecteur :</strong> Une liste ordonn√©e de nombres qui repr√©sente une quantit√© ayant une direction et une magnitude. Les vecteurs sont utilis√©s dans des espaces de haute dimension.</p>
      <p data-cat="algebra"><strong>Autovalues (Valeurs propres) :</strong> Les scalaires associ√©s aux vecteurs propres d'une matrice. Utilis√©es pour comprendre des caract√©ristiques fondamentales des transformations lin√©aires.</p>
      <p data-cat="algebra"><strong>Autovecteurs (Vecteurs propres) :</strong> Les vecteurs associ√©s aux valeurs propres qui ne changent pas de direction lors de la transformation par une matrice.</p>
      <p data-cat="regression"><strong>R√©gularisation :</strong> Technique utilis√©e pour ajouter une p√©nalit√© aux mod√®les statistiques afin d‚Äô√©viter le surapprentissage.</p>
      <p data-cat="regression"><strong>R√©gression Ridge :</strong> Forme de r√©gression lin√©aire qui inclut une r√©gularisation L2 pour p√©naliser les grands coefficients afin de r√©duire le surapprentissage.</p>
      <p data-cat="regression"><strong>R√©gression Lasso :</strong> Forme de r√©gression lin√©aire qui inclut une r√©gularisation L1 pour favoriser la parcimonie en for√ßant certains coefficients √† z√©ro.</p>
      <p data-cat="ml"><strong>Gradient Descent :</strong> Algorithme d'optimisation utilis√© pour minimiser la fonction de co√ªt dans les mod√®les d‚Äôapprentissage machine.</p>
      <p data-cat="proba"><strong>Cha√Æne de Markov :</strong> Mod√®le stochastique d√©crivant une suite d‚Äô√©tats o√π la probabilit√© de passer √† l‚Äô√©tat suivant d√©pend uniquement de l‚Äô√©tat actuel, pas de l‚Äôhistorique complet.</p>
      <p data-cat="stats"><strong>Matrice de corr√©lation :</strong> Une matrice qui montre la corr√©lation entre chaque paire de variables. Utile pour rep√©rer les relations lin√©aires entre les variables.</p>
      <p data-cat="proba"><strong>Loi des grands nombres :</strong> Th√©or√®me indiquant que, pour des variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) avec une variance finie, √† mesure que le nombre d'√©chantillons augmente, la moyenne d‚Äôun √©chantillon converge vers la moyenne th√©orique de la population.</p>
      <p data-cat="stats"><strong>Maximum de vraisemblance (MLE) :</strong> M√©thode d‚Äôestimation des param√®tres d‚Äôun mod√®le statistique consistant √† maximiser la fonction de vraisemblance. C‚Äôest l‚Äôune des approches les plus utilis√©es en statistique.</p>
      <p data-cat="ml"><strong>Algorithme K-means :</strong> Algorithme de clustering non supervis√© qui partitionne les donn√©es en k groupes en minimisant la distance intra-cluster.</p>
      <p data-cat="ml"><strong>R√©seaux bay√©siens :</strong> Structures de donn√©es en forme de graphe qui repr√©sentent les relations probabilistes entre des variables.</p>
      <p data-cat="ml"><strong>K-Nearest Neighbors (K-NN) :</strong> Algorithme utilis√© pour la classification bas√© sur la similarit√© des points voisins dans l'espace des donn√©es.</p>
      <p data-cat="algebra"><strong>Distance euclidienne :</strong> Mesure de la distance directe entre deux points dans un espace multi-dimensionnel. Utilis√©e dans les algorithmes de clustering.</p>
      <p data-cat="stats"><strong>Normalisation et standardisation :</strong> Techniques de pr√©traitement des donn√©es qui ram√®nent les donn√©es dans un intervalle fixe ou une distribution normalis√©e.</p>
      <p data-cat="stats"><strong>Test de Chi-carr√© :</strong> Test statistique utilis√© pour d√©terminer si une relation existe entre deux variables cat√©gorielles.</p>
      <p data-cat="regression"><strong>M√©thode des moindres carr√©s :</strong> M√©thode utilis√©e pour ajuster un mod√®le de r√©gression en minimisant la somme des carr√©s des diff√©rences entre les valeurs observ√©es et pr√©dites.</p>
      <p data-cat="stats"><strong>Variance expliqu√©e :</strong> Proportion de la variance totale d'un ensemble de donn√©es qui est expliqu√©e par un mod√®le statistique.</p>
      <p data-cat="stats"><strong>Effet de levier (Leverage) :</strong> Mesure de l'influence d'un point de donn√©es sur la r√©gression.</p>
      <p data-cat="stats"><strong>Information Entropy (Entropie) :</strong> Mesure de l'incertitude ou de la quantit√© d'information contenue dans une variable al√©atoire.</p>
      <p data-cat="stats"><strong>Mutual Information :</strong> Mesure de la d√©pendance entre deux variables, quantifiant combien conna√Ætre la valeur d'une variable r√©duit l'incertitude de l'autre.</p>
      <p data-cat="ml,stats"><strong>Cross-validation :</strong> Technique utilis√©e pour √©valuer la performance d‚Äôun mod√®le en le testant sur plusieurs sous-ensembles des donn√©es.</p>
      <p data-cat="ml"><strong>Random Forest :</strong> Algorithme bas√© sur la cr√©ation de multiples arbres de d√©cision pour am√©liorer la pr√©cision et r√©duire le risque de surapprentissage.</p>
      <p data-cat="ml"><strong>R√©seaux de neurones convolutifs (CNN) :</strong> Type de r√©seau de neurones sp√©cialis√© dans le traitement des donn√©es structur√©es en grille, comme les images.</p>
      <p data-cat="ml"><strong>R√©seaux de neurones r√©currents (RNN) :</strong> Type de r√©seau de neurones adapt√© aux s√©quences de donn√©es, o√π les informations des √©tapes pr√©c√©dentes influencent les pr√©dictions actuelles.</p>
      <p data-cat="ml"><strong>Courbe ROC (Receiver Operating Characteristic) :</strong> Courbe utilis√©e pour √©valuer les performances d'un mod√®le de classification en tra√ßant le taux de vrais positifs contre le taux de faux positifs.</p>
      <p data-cat="ml"><strong>AUC (Area Under the Curve) :</strong> Surface sous la courbe ROC, utilis√©e comme une mesure de performance globale d‚Äôun mod√®le de classification.</p>
    <p data-cat="ml"><strong>Batch Normalization :</strong> Technique utilis√©e dans les r√©seaux de neurones pour normaliser les activations d'une couche, acc√©l√©rant l'entra√Ænement et am√©liorant la stabilit√©.</p>
<p data-cat="ml"><strong>Dropout :</strong> M√©thode de r√©gularisation pour les r√©seaux de neurones consistant √† d√©sactiver al√©atoirement certains neurones pendant l'entra√Ænement afin de r√©duire le surapprentissage.</p>
<p data-cat="ml"><strong>Ensemble Learning :</strong> Technique combinant plusieurs mod√®les pour am√©liorer la performance globale, comme le bagging, boosting ou stacking.</p>
<p data-cat="ml"><strong>XGBoost :</strong> Impl√©mentation optimis√©e de gradient boosting tr√®s performante pour les t√¢ches de classification et de r√©gression.</p>
<p data-cat="ml"><strong>LightGBM :</strong> Biblioth√®que de gradient boosting optimis√©e pour les gros ensembles de donn√©es, d√©velopp√©e par Microsoft.</p>
<p data-cat="proba"><strong>Rapport de cotes (Odds Ratio) :</strong> Mesure du rapport des chances d‚Äôun √©v√©nement entre deux groupes, couramment utilis√©e en √©pid√©miologie et dans les √©tudes de cas-t√©moins.</p>
<p data-cat="stats"><strong>Coefficient de corr√©lation de Pearson :</strong> Mesure lin√©aire de la relation entre deux variables quantitatives, variant entre -1 et +1.</p>
<p data-cat="stats"><strong>Coefficient de corr√©lation de Spearman :</strong> Mesure non param√©trique de corr√©lation bas√©e sur les rangs des donn√©es.</p>
<p data-cat="proba"><strong>Intervalle de cr√©dibilit√© :</strong> Intervalle estim√© dans lequel un param√®tre a une probabilit√© donn√©e de se trouver, utilis√© en statistique bay√©sienne.</p>
<p data-cat="proba"><strong>Bayes' Theorem :</strong> Formule permettant de calculer la probabilit√© conditionnelle d‚Äôun √©v√©nement √† partir d‚Äôinformations pr√©alables.</p>
<p data-cat="ml"><strong>Feature Engineering :</strong> Processus de cr√©ation, transformation et s√©lection des variables (features) pour am√©liorer la performance d‚Äôun mod√®le.</p>
<p data-cat="ml"><strong>One-Hot Encoding :</strong> Technique de transformation des variables cat√©gorielles en variables binaires utilisables par les algorithmes de machine learning.</p>
<p data-cat="ml"><strong>Word Embeddings :</strong> Repr√©sentations vectorielles des mots, comme Word2Vec ou GloVe, capturant les relations s√©mantiques entre mots.</p>
<p data-cat="stats"><strong>Test t de Student :</strong> Test statistique utilis√© pour comparer la moyenne de deux groupes et d√©terminer si la diff√©rence est significative.</p>
<p data-cat="stats"><strong>ANOVA (Analyse de la variance) :</strong> Test statistique permettant de comparer les moyennes de plusieurs groupes.</p>
<p data-cat="regression"><strong>R√©gression logistique :</strong> Mod√®le statistique utilis√© pour pr√©dire une variable binaire √† partir de variables explicatives.</p>
<p data-cat="regression"><strong>Elastic Net :</strong> Technique de r√©gression combinant les r√©gularisations L1 (Lasso) et L2 (Ridge).</p>
<p data-cat="stats"><strong>Valeur aberrante (Outlier) :</strong> Observation qui s'√©carte fortement des autres valeurs d'un jeu de donn√©es.</p>
<p data-cat="ml"><strong>SMOTE (Synthetic Minority Over-sampling Technique) :</strong> M√©thode de r√©√©chantillonnage synth√©tique pour √©quilibrer des classes d√©s√©quilibr√©es.</p>
<p data-cat="stats"><strong>Kurtosis (Aplatissement) :</strong> Mesure d√©crivant la forme de la distribution, en particulier la concentration des valeurs dans les queues.</p>
<p data-cat="stats"><strong>Skewness (Asym√©trie) :</strong> Mesure indiquant si une distribution est sym√©trique ou penche vers la gauche/droite.</p>
<p data-cat="proba"><strong>Markov Chain :</strong> Mod√®le stochastique d√©crivant une s√©quence d‚Äô√©v√©nements o√π la probabilit√© de chaque √©v√©nement ne d√©pend que de l‚Äô√©tat pr√©c√©dent.</p>
<p data-cat="ml"><strong>Gradient Boosting :</strong> M√©thode d‚Äôensemble qui construit des mod√®les de mani√®re s√©quentielle, chaque nouveau mod√®le corrigeant les erreurs des pr√©c√©dents.</p>
<p data-cat="ml"><strong>CatBoost :</strong> Biblioth√®que de gradient boosting optimis√©e pour les variables cat√©gorielles, d√©velopp√©e par Yandex.</p>
<p data-cat="ml"><strong>Bagging :</strong> Technique d‚Äôensemble qui entra√Æne plusieurs mod√®les ind√©pendamment sur des √©chantillons bootstrap√©s, puis agr√®ge leurs pr√©dictions.</p>
<p data-cat="ml"><strong>AdaBoost :</strong> M√©thode de boosting adaptatif qui pond√®re les erreurs pour se concentrer sur les exemples difficiles √† classer.</p>
<p data-cat="ml"><strong>Hyperparameter Tuning :</strong> Processus d‚Äôoptimisation des param√®tres d‚Äôun mod√®le pour am√©liorer ses performances (grid search, random search, bayesian optimization).</p>
<p data-cat="ml"><strong>Early Stopping :</strong> Technique d'arr√™t anticip√© de l'entra√Ænement d‚Äôun mod√®le lorsque la performance sur un jeu de validation cesse de s'am√©liorer.</p>
<p data-cat="proba"><strong>Processus de Poisson :</strong> Mod√®le stochastique de comptage d√©crivant le nombre d‚Äô√©v√©nements se produisant dans un intervalle de temps fixe, avec des incr√©ments ind√©pendants et un taux constant.</p>
<p data-cat="stats"><strong>Colin√©arit√© :</strong> Situation o√π deux ou plusieurs variables explicatives sont fortement corr√©l√©es, rendant l‚Äôestimation des coefficients instable.</p>
<p data-cat="stats"><strong>Multicolin√©arit√© :</strong> Forme extr√™me de colin√©arit√© impliquant plusieurs variables, pouvant affecter la robustesse des mod√®les lin√©aires.</p>
<p data-cat="proba"><strong>Th√©or√®me central limite (Central Limit Theorem) :</strong> Th√©or√®me stipulant que, pour un grand nombre de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) ayant une variance finie, la distribution de leur somme ou de leur moyenne tend vers une loi normale, quelle que soit la distribution d‚Äôorigine.</p>
<p data-cat="stats"><strong>Analyse discriminante lin√©aire (LDA) :</strong> M√©thode de classification qui projette les donn√©es sur un espace de dimension r√©duite maximisant la s√©paration entre classes.</p>
<p data-cat="regression"><strong>R√©gression polynomiale :</strong> Extension de la r√©gression lin√©aire qui mod√©lise des relations non lin√©aires en introduisant des puissances des variables explicatives.</p>
<p data-cat="regression"><strong>R√©gression PLS (Partial Least Squares) :</strong> M√©thode adapt√©e aux donn√©es fortement corr√©l√©es et aux situations avec plus de variables que d‚Äôobservations.</p>
<p data-cat="algebra"><strong>Norme vectorielle :</strong> Mesure de la taille ou de la longueur d‚Äôun vecteur, comme les normes L1, L2 et L‚àû.</p>
<p data-cat="algebra"><strong>Produit scalaire :</strong> Op√©ration entre deux vecteurs donnant un nombre, repr√©sentant la projection d‚Äôun vecteur sur un autre.</p>
<p data-cat="algebra"><strong>Produit matriciel :</strong> Op√©ration qui combine deux matrices pour produire une troisi√®me, utilis√©e dans de nombreuses transformations lin√©aires.</p>
<p data-cat="algebra"><strong>D√©composition en valeurs singuli√®res (SVD) :</strong> Factorisation d‚Äôune matrice en trois matrices, utilis√©e pour la r√©duction de dimension et la compression.</p>
<p data-cat="ml"><strong>Apprentissage par transfert (Transfer Learning) :</strong> R√©utilisation d‚Äôun mod√®le pr√©-entra√Æn√© sur un probl√®me similaire pour acc√©l√©rer l‚Äôapprentissage sur un nouveau probl√®me.</p>
<p data-cat="ml"><strong>Apprentissage semi-supervis√© :</strong> Technique exploitant √† la fois des donn√©es √©tiquet√©es et non √©tiquet√©es pour am√©liorer les performances d‚Äôun mod√®le.</p>
<p data-cat="ml"><strong>Apprentissage auto-supervis√© :</strong> Technique o√π un mod√®le g√©n√®re ses propres labels √† partir des donn√©es brutes pour pr√©-entra√Æner ses repr√©sentations.</p>
<p data-cat="ml"><strong>GAN (Generative Adversarial Network) :</strong> Architecture de deep learning compos√©e d‚Äôun g√©n√©rateur et d‚Äôun discriminateur, utilis√©e pour cr√©er des donn√©es synth√©tiques r√©alistes.</p>
<p data-cat="ml"><strong>Autoencoder :</strong> R√©seau de neurones entra√Æn√© √† reconstruire ses entr√©es apr√®s les avoir compress√©es, utilis√© pour la r√©duction de dimension et la d√©tection d‚Äôanomalies.</p>
<p data-cat="proba"><strong>Fonction de densit√© de probabilit√© (PDF) :</strong> Fonction qui d√©crit la r√©partition d‚Äôune variable al√©atoire continue. La probabilit√© que la variable prenne une valeur dans un intervalle donn√© correspond √† l‚Äôint√©grale de cette densit√© sur cet intervalle.</p>
<p data-cat="proba"><strong>Fonction de masse de probabilit√© (PMF) :</strong> Fonction qui associe √† chaque valeur possible d‚Äôune variable al√©atoire discr√®te la probabilit√© que cette valeur se r√©alise.</p><p data-cat="proba"><strong>Loi uniforme :</strong> Loi de probabilit√© continue o√π toutes les valeurs d‚Äôun intervalle ont la m√™me probabilit√©.</p>
<p data-cat="proba"><strong>Loi log-normale :</strong> Loi de probabilit√© continue pour une variable dont le logarithme suit une loi normale ; utilis√©e pour mod√©liser des ph√©nom√®nes multiplicatifs.</p>
<p data-cat="proba"><strong>Loi du chi-deux (œá¬≤) :</strong> Loi de probabilit√© continue, d√©finie uniquement pour des valeurs positives, utilis√©e dans les tests du chi-deux et l‚Äôestimation de variance, param√©tr√©e par ses degr√©s de libert√©.</p>
<p data-cat="proba"><strong>Loi de Student (t) :</strong> Loi de probabilit√© continue utilis√©e pour estimer ou tester une moyenne lorsque la taille d‚Äô√©chantillon est petite et que la variance de la population est inconnue.</p>
<p data-cat="proba"><strong>Loi de Fisher-Snedecor (F) :</strong> Loi de probabilit√© continue utilis√©e pour comparer des variances dans les tests d'analyse de variance (ANOVA).</p>
<p data-cat="proba"><strong>Loi b√™ta :</strong> Loi de probabilit√© continue d√©finie sur [0, 1], param√©tr√©e par deux coefficients positifs, utilis√©e pour mod√©liser des proportions.</p>
<p data-cat="proba"><strong>Loi gamma :</strong> Loi de probabilit√© continue utilis√©e pour mod√©liser des temps d‚Äôattente ou des dur√©es, param√©tr√©e par un coefficient de forme et un coefficient de taux (ou d‚Äô√©chelle).</p>
<p data-cat="proba"><strong>Loi binomiale :</strong> Loi de probabilit√© discr√®te qui donne la probabilit√© d'obtenir un nombre donn√© de succ√®s dans un nombre fixe d'essais ind√©pendants.</p>
<p data-cat="proba"><strong>Loi g√©om√©trique :</strong> Loi de probabilit√© discr√®te qui donne la probabilit√© que le premier succ√®s survienne au k·µâ essai, pour des essais ind√©pendants avec probabilit√© de succ√®s constante.</p>
<p data-cat="proba"><strong>Loi hyperg√©om√©trique :</strong> Loi de probabilit√© discr√®te qui mod√©lise le nombre de succ√®s dans un √©chantillon tir√© sans remise d'une population finie.</p>
<p data-cat="proba"><strong>Loi binomiale n√©gative :</strong> Loi de probabilit√© discr√®te qui mod√©lise le nombre d‚Äô√©checs avant d‚Äôobtenir un nombre fix√© de succ√®s, pour des essais ind√©pendants avec probabilit√© de succ√®s constante.</p>
<p data-cat="proba"><strong>Loi exponentielle :</strong> Loi de probabilit√© continue qui mod√©lise le temps entre deux √©v√©nements dans un processus √† taux constant.</p>
<p data-cat="stats"><strong>M√©diane :</strong> Valeur qui s√©pare un ensemble de donn√©es ordonn√©es en deux parties de m√™me taille. Robuste aux valeurs extr√™mes, elle est souvent utilis√©e comme mesure centrale alternative √† la moyenne.</p>
<p data-cat="stats"><strong>Mode :</strong> Valeur ou cat√©gorie la plus fr√©quente dans un ensemble de donn√©es. Utile pour les donn√©es qualitatives et quantitatives discr√®tes.</p>
<p data-cat="stats"><strong>IQR (Intervalle inter-quartile) :</strong> Diff√©rence entre le troisi√®me quartile (Q3) et le premier quartile (Q1). Mesure robuste de dispersion, insensible aux valeurs aberrantes.</p>
<p data-cat="stats"><strong>MAD (Median Absolute Deviation) :</strong> M√©diane des valeurs absolues des √©carts √† la m√©diane. Indicateur robuste de la dispersion des donn√©es.</p>
<p data-cat="stats"><strong>Z-score :</strong> Valeur standardis√©e indiquant combien d‚Äô√©carts-types une observation se situe par rapport √† la moyenne, permettant de comparer des valeurs issues de distributions diff√©rentes.</p>
<p data-cat="stats"><strong>Taille d‚Äôeffet (Cohen‚Äôs d) :</strong> Mesure normalis√©e de la diff√©rence entre deux moyennes, exprim√©e en nombre d‚Äô√©carts-types. Permet d‚Äô√©valuer l‚Äôimportance pratique d‚Äôun effet au-del√† de la significativit√© statistique.</p>
<p data-cat="stats"><strong>Taille d‚Äôeffet (r) :</strong> Mesure bas√©e sur un coefficient de corr√©lation ou extraite de statistiques de test, exprimant la force de l‚Äôassociation entre deux variables.</p>
<p data-cat="stats"><strong>Puissance statistique :</strong> Probabilit√© qu‚Äôun test d√©tecte un effet r√©el (1 ‚àí Œ≤). D√©pend de la taille de l‚Äôeffet, de l‚Äô√©chantillon et du seuil Œ± choisi.</p>
<p data-cat="stats"><strong>Erreur de type I (Œ±) :</strong> Rejeter l‚Äôhypoth√®se nulle alors qu‚Äôelle est vraie. Correspond au risque de faux positif, fix√© par le seuil de significativit√©.</p>
<p data-cat="stats"><strong>Erreur de type II (Œ≤) :</strong> Ne pas rejeter l‚Äôhypoth√®se nulle alors qu‚Äôelle est fausse. Correspond au risque de faux n√©gatif.</p>
<p data-cat="stats"><strong>Tests multiples :</strong> Ensemble de proc√©dures pour ajuster le seuil de significativit√© lorsque plusieurs tests sont effectu√©s, afin de limiter l‚Äôinflation des faux positifs (ex. Bonferroni, Benjamini‚ÄìHochberg).</p>
<p data-cat="stats"><strong>H√©t√©rosc√©dasticit√© :</strong> Situation o√π la variance des r√©sidus n‚Äôest pas constante √† travers les valeurs des pr√©dicteurs. Pose probl√®me pour les mod√®les lin√©aires classiques.</p>
<p data-cat="stats"><strong>Bo√Æte √† moustaches :</strong> Repr√©sentation graphique r√©sumant la m√©diane, l‚ÄôIQR et les valeurs extr√™mes. Utilis√©e pour visualiser la dispersion et d√©tecter les valeurs aberrantes.</p>
<p data-cat="stats"><strong>QQ-plot :</strong> Graphique comparant les quantiles d‚Äôun √©chantillon avec ceux d‚Äôune distribution th√©orique, souvent utilis√© pour v√©rifier l‚Äôhypoth√®se de normalit√©.</p>
<p data-cat="proba"><strong>Probabilit√© conditionnelle :</strong> Probabilit√© qu‚Äôun √©v√©nement A se produise sachant qu‚Äôun √©v√©nement B est r√©alis√©. Not√©e P(A|B) et d√©finie par P(A‚à©B)/P(B).</p>
<p data-cat="proba"><strong>Ind√©pendance :</strong> Deux √©v√©nements A et B sont ind√©pendants si la r√©alisation de l‚Äôun n‚Äôinfluence pas la probabilit√© de l‚Äôautre, soit P(A‚à©B)=P(A)P(B).</p>
<p data-cat="proba"><strong>Loi multinomiale :</strong> Extension de la loi binomiale √† k issues possibles par essai. Mod√©lise la probabilit√© d‚Äôobtenir des effectifs sp√©cifiques sur n essais ind√©pendants.</p>
<p data-cat="proba"><strong>CDF (Fonction de r√©partition) :</strong> Fonction donnant, pour toute valeur x, la probabilit√© que la variable al√©atoire soit inf√©rieure ou √©gale √† x. Not√©e F(x)=P(X‚â§x).</p>
<p data-cat="proba"><strong>Quantiles / Percentiles :</strong> Valeurs seuils qui divisent une distribution en intervalles contenant des proportions sp√©cifiques des observations.</p>
<p data-cat="proba"><strong>Fonction g√©n√©ratrice des moments (MGF) :</strong> Fonction M(t)=E[e^{tX}] qui, si elle existe, permet d‚Äôobtenir tous les moments d‚Äôune variable al√©atoire en d√©rivant M(t) en t=0.</p>
<p data-cat="proba"><strong>Fonction caract√©ristique :</strong> Fonction œÜ(t)=E[e^{itX}] toujours d√©finie, utilis√©e notamment pour √©tudier les sommes de variables al√©atoires et la convergence en loi.</p>
<p data-cat="proba"><strong>In√©galit√© de Markov :</strong> Pour une variable al√©atoire positive X, P(X‚â•a) ‚â§ E[X]/a. Fournit une borne de probabilit√© sans hypoth√®se forte sur la distribution.</p>
<p data-cat="proba"><strong>In√©galit√© de Tchebychev :</strong> Pour toute variable X de moyenne Œº et variance œÉ¬≤, P(|X‚àíŒº|‚â•kœÉ) ‚â§ 1/k¬≤. Encadre la dispersion autour de la moyenne.</p>
<p data-cat="proba"><strong>In√©galit√© de Jensen :</strong> Pour une fonction convexe f, f(E[X]) ‚â§ E[f(X)]. Utilis√©e en optimisation et th√©orie des probabilit√©s.</p>
<p data-cat="proba"><strong>Variables continues :</strong> Variables pouvant prendre une infinit√© de valeurs r√©elles dans un intervalle, mod√©lis√©es par une PDF.</p>
<p data-cat="proba"><strong>Variables discr√®tes :</strong> Variables ne pouvant prendre qu‚Äôun ensemble d√©nombrable de valeurs, mod√©lis√©es par une PMF.</p>
<p data-cat="regression"><strong>Hypoth√®ses OLS :</strong> Ensemble de conditions n√©cessaires pour que les estimateurs de la r√©gression lin√©aire soient optimaux : relation lin√©aire entre variables, ind√©pendance des r√©sidus, normalit√© des r√©sidus, variance constante (homosc√©dasticit√©) et absence de multicolin√©arit√© s√©v√®re.</p>
<p data-cat="regression"><strong>R¬≤ :</strong> Coefficient de d√©termination mesurant la proportion de variance de la variable d√©pendante expliqu√©e par le mod√®le. Une valeur proche de 1 indique un ajustement √©lev√©.</p>
<p data-cat="regression"><strong>R¬≤ ajust√© :</strong> Version du R¬≤ qui p√©nalise les mod√®les avec de nombreuses variables non pertinentes, permettant de comparer des mod√®les avec un nombre de pr√©dicteurs diff√©rent.</p>
<p data-cat="regression"><strong>Diagnostics des r√©sidus :</strong> Analyse graphique ou statistique des r√©sidus pour v√©rifier les hypoth√®ses de la r√©gression (normalit√©, homosc√©dasticit√©, ind√©pendance) et d√©tecter d‚Äô√©ventuelles anomalies.</p>
<p data-cat="regression"><strong>Durbin‚ÄìWatson :</strong> Statistique utilis√©e pour d√©tecter l‚Äôautocorr√©lation des r√©sidus dans les mod√®les de r√©gression appliqu√©s √† des donn√©es ordonn√©es dans le temps.</p>
<p data-cat="regression"><strong>VIF (Variance Inflation Factor) :</strong> Indicateur mesurant le degr√© de multicolin√©arit√© d‚Äôune variable explicative avec les autres. Un VIF √©lev√© indique une corr√©lation forte avec d‚Äôautres variables du mod√®le.</p>
<p data-cat="regression"><strong>GLM (Generalized Linear Model) :</strong> Famille de mod√®les statistiques g√©n√©ralisant la r√©gression lin√©aire en permettant une distribution de r√©ponse autre que normale et en utilisant une fonction de lien adapt√©e (logit, log, identit√©, etc.).</p>
<p data-cat="regression"><strong>Fonctions de perte :</strong> Fonctions mesurant l‚Äô√©cart entre les valeurs pr√©dites et observ√©es, comme MSE (erreur quadratique moyenne), MAE (erreur absolue moyenne) ou Log-loss (perte logarithmique pour les probabilit√©s).</p>
<p data-cat="regression"><strong>Splines :</strong> Fonctions polynomiales par morceaux utilis√©es pour mod√©liser des relations non lin√©aires tout en assurant la continuit√© et la douceur aux points de jonction.</p>
<p data-cat="regression"><strong>Termes d‚Äôinteraction :</strong> Variables cr√©√©es en multipliant deux pr√©dicteurs afin de capturer l‚Äôeffet combin√© de ces variables sur la variable cible.</p>
<p data-cat="ml"><strong>Split train/val/test :</strong> Division d‚Äôun jeu de donn√©es en trois ensembles : entra√Ænement (train) pour ajuster le mod√®le, validation (val) pour optimiser les hyperparam√®tres, et test pour √©valuer la performance finale sur des donn√©es in√©dites.</p>
<p data-cat="ml"><strong>Validation crois√©e k-fold :</strong> Technique d‚Äô√©valuation consistant √† diviser les donn√©es en k sous-ensembles, √† entra√Æner le mod√®le sur k-1 d‚Äôentre eux et √† le tester sur le sous-ensemble restant, en r√©p√©tant l‚Äôop√©ration k fois pour obtenir une performance moyenne.</p>
<p data-cat="ml"><strong>Pr√©cision (accuracy) :</strong> Proportion de pr√©dictions correctes sur l‚Äôensemble des pr√©dictions effectu√©es par un mod√®le. Pertinente lorsque les classes sont √©quilibr√©es.</p>
<p data-cat="ml"><strong>Rappel (recall) :</strong> Proportion de vrais positifs d√©tect√©s parmi tous les positifs r√©els. Indique la capacit√© du mod√®le √† identifier les exemples pertinents.</p>
<p data-cat="ml"><strong>Sp√©cificit√© :</strong> Proportion de vrais n√©gatifs correctement identifi√©s parmi tous les n√©gatifs r√©els. Utile pour √©valuer la capacit√© √† √©viter les faux positifs.</p>
<p data-cat="ml"><strong>F1 :</strong> Moyenne harmonique de la pr√©cision et du rappel. Favorise un √©quilibre entre les deux mesures, notamment en cas de classes d√©s√©quilibr√©es.</p>
<p data-cat="ml"><strong>Matrice de co√ªts :</strong> Tableau attribuant un co√ªt sp√©cifique √† chaque type d‚Äôerreur ou de bonne classification, permettant d‚Äôadapter la d√©cision du mod√®le aux enjeux m√©tier.</p>
<p data-cat="ml"><strong>Courbe PR & AUPRC :</strong> Courbe repr√©sentant la pr√©cision en fonction du rappel pour diff√©rents seuils, et AUPRC √©tant l‚Äôaire sous cette courbe. Utile pour les jeux de donn√©es tr√®s d√©s√©quilibr√©s.</p>
<p data-cat="ml"><strong>Calibration des probabilit√©s :</strong> Processus visant √† ajuster les probabilit√©s pr√©dites par un mod√®le afin qu‚Äôelles refl√®tent correctement les fr√©quences observ√©es dans les donn√©es r√©elles.</p>
<p data-cat="ml"><strong>Seuil de d√©cision :</strong> Valeur limite appliqu√©e aux probabilit√©s pr√©dites pour d√©terminer la classe attribu√©e par le mod√®le. Peut √™tre ajust√© pour √©quilibrer pr√©cision et rappel.</p>
<p data-cat="ml"><strong>SVM (Support Vector Machine) :</strong> Algorithme de classification qui cherche √† maximiser la marge entre les classes, avec possibilit√© d‚Äôutiliser des noyaux pour traiter des s√©parations non lin√©aires.</p>
<p data-cat="ml"><strong>Na√Øve Bayes :</strong> Classifieur probabiliste bas√© sur le th√©or√®me de Bayes, supposant l‚Äôind√©pendance conditionnelle entre les variables explicatives.</p>
<p data-cat="ml"><strong>Arbre de d√©cision :</strong> Mod√®le pr√©dictif structur√© en n≈ìuds de d√©cision et feuilles de pr√©diction, permettant une interpr√©tation simple des r√®gles d‚Äôattribution des classes.</p>
<p data-cat="ml"><strong>Clustering hi√©rarchique :</strong> M√©thode de regroupement des donn√©es consistant √† fusionner ou diviser les clusters de mani√®re it√©rative pour former une hi√©rarchie repr√©sent√©e par un dendrogramme.</p>
<p data-cat="ml"><strong>DBSCAN :</strong> Algorithme de clustering bas√© sur la densit√©, capable de d√©tecter des groupes de formes quelconques et de g√©rer les points bruit√©s.</p>
<p data-cat="ml"><strong>Indice de silhouette :</strong> Mesure de la qualit√© d‚Äôun clustering, comparant la coh√©sion interne d‚Äôun cluster √† sa s√©paration avec les autres clusters.</p>
<p data-cat="ml"><strong>t-SNE :</strong> Technique de r√©duction de dimensionnalit√© non lin√©aire, optimis√©e pour la visualisation en 2D ou 3D des structures locales des donn√©es.</p>
<p data-cat="ml"><strong>UMAP :</strong> M√©thode de r√©duction de dimensionnalit√© rapide, conservant √† la fois la structure locale et globale des donn√©es.</p>
<p data-cat="ml"><strong>SHAP :</strong> M√©thode d‚Äôexplicabilit√© attribuant √† chaque variable une contribution √† la pr√©diction, bas√©e sur les valeurs de Shapley issues de la th√©orie des jeux.</p>
<p data-cat="ml"><strong>LIME :</strong> Technique d‚Äôexplication locale approximant un mod√®le complexe par un mod√®le lin√©aire interpr√©table dans la zone d‚Äôun exemple particulier.</p>
<p data-cat="ml"><strong>Pipelines :</strong> Cha√Ænes ordonn√©es d‚Äô√©tapes de pr√©traitement et de mod√©lisation permettant d‚Äôautomatiser et de s√©curiser l‚Äôentra√Ænement d‚Äôun mod√®le.</p>
<p data-cat="ml"><strong>Fuite de donn√©es (data leakage) :</strong> Introduction involontaire d‚Äôinformations provenant des donn√©es de test dans l‚Äôentra√Ænement, faussant les performances estim√©es du mod√®le.</p>
<p data-cat="algebra"><strong>Rang :</strong> Nombre maximum de colonnes ou de lignes lin√©airement ind√©pendantes dans une matrice. Indique la dimension de l‚Äôespace engendr√© par ses colonnes ou ses lignes.</p>
<p data-cat="algebra"><strong>D√©terminant :</strong> Scalaire calcul√© √† partir d‚Äôune matrice carr√©e, indiquant le facteur d‚Äô√©chelle d‚Äôune transformation lin√©aire et permettant de savoir si la matrice est inversible (d√©terminant nul ‚ü∫ matrice non inversible).</p>
<p data-cat="algebra"><strong>Trace :</strong> Somme des √©l√©ments diagonaux d‚Äôune matrice carr√©e, √©quivalente √† la somme de ses valeurs propres. Utilis√©e en statistiques et en optimisation.</p>
<p data-cat="algebra"><strong>Matrice d√©finie positive :</strong> Matrice sym√©trique pour laquelle x·µÄAx > 0 pour tout vecteur non nul x. Garantit que la forme quadratique associ√©e est strictement positive.</p>
<p data-cat="algebra"><strong>Semi-d√©finie positive :</strong> Matrice sym√©trique pour laquelle x·µÄAx ‚â• 0 pour tout vecteur x. Les matrices de covariance sont typiquement semi-d√©finies positives.</p>
<p data-cat="algebra"><strong>Orthogonalit√© :</strong> Relation entre deux vecteurs dont le produit scalaire est nul. Implique que les vecteurs sont perpendiculaires dans l‚Äôespace vectoriel.</p>
<p data-cat="algebra"><strong>Base orthonorm√©e :</strong> Ensemble de vecteurs unitaires et mutuellement orthogonaux formant une base d‚Äôun espace vectoriel. Simplifie les calculs de projection et de transformation.</p>
<p data-cat="algebra"><strong>Projection :</strong> Transformation d‚Äôun vecteur sur un sous-espace, conservant la composante parall√®le et √©liminant la composante orthogonale.</p>
<p data-cat="algebra"><strong>D√©composition LU :</strong> Factorisation d‚Äôune matrice en produit d‚Äôune matrice triangulaire inf√©rieure (L) et d‚Äôune matrice triangulaire sup√©rieure (U), utilis√©e pour r√©soudre efficacement des syst√®mes d‚Äô√©quations lin√©aires.</p>
<p data-cat="algebra"><strong>Cholesky :</strong> D√©composition d‚Äôune matrice sym√©trique d√©finie positive en produit LL·µÄ, o√π L est triangulaire inf√©rieure. M√©thode rapide et stable pour l‚Äôinversion et la r√©solution de syst√®mes.</p>
<p data-cat="algebra"><strong>Gradient :</strong> Vecteur des d√©riv√©es partielles d‚Äôune fonction multivariable, pointant dans la direction de la plus forte augmentation de la fonction.</p>
<p data-cat="algebra"><strong>Jacobien :</strong> Matrice contenant les d√©riv√©es partielles premi√®res d‚Äôun vecteur-fonction par rapport √† ses variables. D√©crit les variations locales d‚Äôune transformation multivariable.</p>
<p data-cat="algebra"><strong>Hessien :</strong> Matrice des d√©riv√©es secondes d‚Äôune fonction scalaire multivariable, d√©crivant la courbure locale. Utilis√©e pour analyser la convexit√© et optimiser les fonctions.</p>
<p data-cat="timeseries"><strong>Stationnarit√© :</strong> Propri√©t√© d‚Äôune s√©rie temporelle dont la distribution statistique (moyenne, variance, autocorr√©lation) reste constante dans le temps. Condition souvent n√©cessaire pour certains mod√®les.</p>
<p data-cat="timeseries"><strong>Saisonnalit√© :</strong> Pr√©sence de motifs qui se r√©p√®tent √† intervalles r√©guliers (jour, semaine, ann√©e). Courante dans les donn√©es √©conomiques ou m√©t√©orologiques.</p>
<p data-cat="timeseries"><strong>Tendance :</strong> Composante √† long terme de la s√©rie, repr√©sentant une hausse ou une baisse globale sur la p√©riode observ√©e.</p>
<p data-cat="timeseries"><strong>ACF (Autocorrelation Function) :</strong> Fonction mesurant la corr√©lation entre la s√©rie et ses propres valeurs d√©cal√©es dans le temps. Utilis√©e pour d√©tecter les d√©pendances temporelles.</p>
<p data-cat="timeseries"><strong>PACF (Partial Autocorrelation Function) :</strong> Mesure de la corr√©lation entre la s√©rie et ses retards, en √©liminant l‚Äôeffet des retards interm√©diaires. Sert √† identifier l‚Äôordre d‚Äôun mod√®le AR.</p>
<p data-cat="timeseries"><strong>AR(p) :</strong> Mod√®le autor√©gressif o√π la valeur actuelle d√©pend d‚Äôune combinaison lin√©aire de ses p valeurs pass√©es et d‚Äôun terme d‚Äôerreur al√©atoire.</p>
<p data-cat="timeseries"><strong>MA(q) :</strong> Mod√®le √† moyenne mobile o√π la valeur actuelle d√©pend d‚Äôune combinaison lin√©aire des q derni√®res erreurs al√©atoires.</p>
<p data-cat="timeseries"><strong>ARMA(p,q) :</strong> Mod√®le combinant une composante autor√©gressive (AR) et une composante √† moyenne mobile (MA), adapt√© aux s√©ries stationnaires.</p>
<p data-cat="timeseries"><strong>ARIMA(p,d,q) :</strong> Extension d‚ÄôARMA int√©grant une diff√©renciation d‚Äôordre d pour rendre la s√©rie stationnaire. Tr√®s utilis√© en pr√©vision.</p>
<p data-cat="timeseries"><strong>Lissage exponentiel :</strong> M√©thode de pr√©vision donnant plus de poids aux observations r√©centes via un facteur d‚Äôatt√©nuation exponentiel.</p>
<p data-cat="timeseries"><strong>ETS :</strong> Famille de mod√®les combinant Erreur, Tendance et Saisonnalit√©, de mani√®re additive ou multiplicative, pour mod√©liser des s√©ries temporelles.</p>
<p data-cat="abtest"><strong>Randomisation :</strong> Attribution al√©atoire des sujets ou unit√©s aux groupes de traitement et de contr√¥le, afin de r√©duire les biais et √©quilibrer les facteurs confondants.</p>
<p data-cat="abtest"><strong>Groupe contr√¥le :</strong> Ensemble de sujets ne recevant pas l‚Äôintervention test√©e, servant de r√©f√©rence pour mesurer l‚Äôeffet du traitement.</p>
<p data-cat="abtest"><strong>Effet traitement vs effet levier :</strong> Diff√©rencier l‚Äôimpact r√©el de l‚Äôintervention de simples effets li√©s √† l‚Äôexposition ou √† la s√©lection des participants.</p>
<p data-cat="abtest"><strong>P-hacking :</strong> Manipulation des analyses statistiques ou s√©lection opportuniste des donn√©es pour obtenir artificiellement des r√©sultats significatifs.</p>
<p data-cat="abtest"><strong>Test s√©quentiel :</strong> M√©thode permettant d‚Äôanalyser les r√©sultats √† intervalles r√©guliers pendant l‚Äôexp√©rimentation, avec ajustement du seuil statistique pour contr√¥ler le risque d‚Äôerreur de type I.</p>
<p data-cat="abtest"><strong>Taille d‚Äô√©chantillon :</strong> Nombre minimal d‚Äôobservations n√©cessaire pour garantir une puissance statistique suffisante et d√©tecter un effet d‚Äôampleur donn√©e.</p>
<p data-cat="proba"><strong>Th√©or√®me central limite (Central Limit Theorem) :</strong> Th√©or√®me stipulant que, pour un grand nombre de variables al√©atoires ind√©pendantes et identiquement distribu√©es (i.i.d.) ayant une variance finie, la distribution de leur somme ou de leur moyenne tend vers une loi normale, quelle que soit la distribution d‚Äôorigine.</p>

    </div>
  </div>
</section>
 <!-- Palette de commandes (‚åòK / Ctrl‚ÄëK) -->
<div id="cmdk" class="cmdk" role="dialog" aria-modal="true" hidden>
  <div class="cmdk-box">
    <input id="cmdk-input" type="search" placeholder="Rechercher (sections & termes)‚Ä¶" aria-label="Palette de commandes" />
    <ul id="cmdk-results" class="cmdk-results" role="listbox" aria-label="R√©sultats"></ul>
    <div class="cmdk-hint">‚Üë‚Üì pour naviguer ‚Ä¢ Entr√©e pour ouvrir ‚Ä¢ Tab pour auto‚Äëcompl√©ter ‚Ä¢ √âchap pour fermer</div>
  </div>
</div>

<!-- Zone toasts -->
<div id="toast-root" aria-live="polite" aria-atomic="true"></div>


 <section id="simulations" class="reveal d2" hidden>
  <div class="card">
    <h2 class="h1">Simulations interactives</h2>
    <p class="sub">Choisis une loi, r√®gle les param√®tres et observe sa PDF (continue) ou PMF (discr√®te).</p>

    <div class="sim-toolbar">
      <label class="sim-field">
        <span>Distribution</span>
        <select id="simDist">
          <!-- Continues -->
          <option value="normal">Normale (Œº, œÉ)</option>
          <option value="uniform">Uniforme [a, b]</option>
          <option value="lognormal">Log-normale (Œº, œÉ)</option>
          <option value="chisq">Chi-deux (ŒΩ)</option>
          <option value="student">Student t (ŒΩ)</option>
          <option value="f">Fisher F (ŒΩ‚ÇÅ, ŒΩ‚ÇÇ)</option>
          <option value="beta">Beta (Œ±, Œ≤)</option>
          <option value="gamma">Gamma (k, Œª)</option>
          <option disabled>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</option>
          <!-- Discr√®tes -->
          <option value="bernoulli">Bernoulli (p)</option>
          <option value="binomial">Binomiale (n, p)</option>
          <option value="poisson">Poisson (Œª)</option>
          <option value="geometric">G√©om√©trique (p)</option>
          <option value="hypergeo">Hyperg√©om√©trique (N, K, n)</option>
          <option value="nbinom">Binomiale n√©gative (r, p)</option>
          <option value="exponential">Exponentielle (Œª)</option>
        </select>
      </label>

      <div id="simParams" class="sim-params"><!-- param√®tres inject√©s --></div>

      <div class="sim-actions">
        <button class="btn" id="simReset">R√©initialiser</button>
        <button class="btn" id="simAnimate">Animer</button>
      </div>
    </div>

    <div class="sim-chart" style="height: 360px;">
      <canvas id="simChart" aria-label="Graphique de la distribution"></canvas>
    </div>

    <div class="sim-stats">
      <div class="stat"><span class="label">Moyenne</span><span id="simMean" class="val">‚Äî</span></div>
      <div class="stat"><span class="label">Variance</span><span id="simVar" class="val">‚Äî</span></div>
      <div class="stat"><span class="label">Type</span><span id="simType" class="val">‚Äî</span></div>
    </div>
  </div>
</section>
    <section id="plan" class="reveal d2" hidden>
      <div class="card"><h2 class="h1">Plan du b√¢timent principal</h2><img src="Plan_salles.jpg" alt="Plan du b√¢timent principal" style="width:100%; border-radius:14px; border:1px solid var(--card-border)"></div>
    </section>


    <footer class="reveal d3">¬© SINGH 2025 ‚Äì 2026 ‚Ä¢ Universit√© Paris Cit√©</footer>
  </main>

  <script src="script.js"></script>
</body>
</html>
